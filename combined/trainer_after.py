import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from PIL import Image
import torchvision.utils as vutils

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import config as c
from data import StegoDataset
from Main import (
    PSNR, SSIM, attack_image, DCGANGenerator,
    GenerativeErrorPredictor, DiscriminatorRefineHead,
    DynamicWeightBalancer, Discriminator  # ä»Mainå¯¼å…¥Discriminatorï¼Œä¸å†è‡ªå®šä¹‰
)
from model import (
    EnhancedPRIS, PZMsFeatureExtractor,
    ReversibleBlock, FeatureFusionModule
)

class Trainer:
    def __init__(self):
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # æ¨¡å‹åˆå§‹åŒ–
        self.gen = EnhancedPRIS().to(self.device)  # éšå†™ç”Ÿæˆå™¨
        self.disc = Discriminator(in_channels=3).to(self.device)  # ä½¿ç”¨Mainçš„Discriminator
        self.error_predictor = GenerativeErrorPredictor(in_channels=3).to(self.device)  # ä¼ å…¥in_channelsåŒ¹é…
        self.refine_head = DiscriminatorRefineHead(disc=self.disc).to(self.device)  # ä¿®å¤å¤´
        self.pzms_extractor = PZMsFeatureExtractor(max_order=c.pzms_max_order).to(self.device)  # PZMsæå–å™¨
        # åŠ¨æ€æƒé‡å¹³è¡¡å™¨ï¼ˆæ”¯æŒä¼ å…¥å½“å‰æŸå¤±å€¼ï¼‰
        self.weight_balancer = DynamicWeightBalancer(
            base_weights={
                'cap': c.base_capacity_weight,
                'rob': c.base_robustness_weight,
                'imp': c.base_imperceptibility_weight
            }
        )
        self.training = True
        # ä¼˜åŒ–å™¨ï¼šæé«˜åˆ¤åˆ«å™¨å­¦ä¹ ç‡ï¼ˆè§£å†³åˆ¤åˆ«å™¨æ— æ•ˆé—®é¢˜ï¼‰
        self.opt_gen = optim.Adam(self.gen.parameters(), lr=c.lr_gen, betas=c.betas)
        self.opt_disc = optim.Adam(self.disc.parameters(), lr=c.lr_disc * 2, betas=c.betas)  # åˆ¤åˆ«å™¨å­¦ä¹ ç‡Ã—2

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆè§£å†³å­¦ä¹ ç‡å›ºå®šé—®é¢˜ï¼‰
        self.scheduler_gen = optim.lr_scheduler.StepLR(
            self.opt_gen, step_size=c.lr_step_size, gamma=c.lr_gamma
        )
        self.scheduler_disc = optim.lr_scheduler.StepLR(
            self.opt_disc, step_size=c.lr_step_size, gamma=c.lr_gamma
        )

        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCELoss()

        # æ•°æ®é›†å’ŒåŠ è½½å™¨ï¼ˆWindowsä¸‹num_workers=0é¿å…å¤šè¿›ç¨‹é”™è¯¯ï¼‰
        self.train_dataset = StegoDataset(is_train=True)
        self.val_dataset = StegoDataset(is_train=False)
        self.train_loader = DataLoader(
            self.train_dataset, batch_size=c.batch_size, shuffle=True, num_workers=0
        )
        self.val_loader = DataLoader(
            self.val_dataset, batch_size=c.batchsize_val, shuffle=c.shuffle_val, num_workers=0
        )

        # æ—¥å¿—å’Œæ£€æŸ¥ç‚¹
        self.writer = SummaryWriter(c.LOG_PATH)
        os.makedirs(c.CHECKPOINT_PATH, exist_ok=True)
        os.makedirs(c.IMAGE_PATH, exist_ok=True)  # ç¤ºä¾‹å›¾ç‰‡ä¿å­˜ç›®å½•
        self.best_psnr = 0.0  # æœ€ä½³éªŒè¯PSNRï¼ˆç§˜å¯†æå–ï¼‰
        self.best_psnr_container = 0.0  # æœ€ä½³éªŒè¯PSNRï¼ˆå®¹å™¨ä¸å®¿ä¸»ï¼‰
        # æ–°å¢ï¼šPSNRæå‡é˜ˆå€¼ï¼ˆç”¨äºæ—©åœå’Œæƒé‡è°ƒæ•´ï¼‰
        self.psnr_improve_threshold = 1.02  # è‡³å°‘æå‡2%æ‰è§†ä¸ºæœ‰æ•ˆæå‡

    def get_feature_matching_loss(self, real_feats, fake_feats):
        """è®¡ç®—ç‰¹å¾åŒ¹é…æŸå¤±ï¼ˆè§£å†³åˆ¤åˆ«å™¨æ— æ•ˆé—®é¢˜ï¼‰"""
        loss = 0.0
        for r_feat, f_feat in zip(real_feats, fake_feats):
            loss += torch.mean(torch.abs(r_feat - f_feat))
        return loss / len(real_feats)

    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆæœ€ä½³æ¨¡å‹æˆ–æ–­ç‚¹ç»­è®­ï¼‰"""
        if os.path.exists(checkpoint_path):
            # åŠ è½½æ£€æŸ¥ç‚¹å¹¶æ˜ å°„åˆ°å½“å‰è®¾å¤‡
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.gen.load_state_dict(checkpoint['gen_state_dict'])
            self.disc.load_state_dict(checkpoint['disc_state_dict'])
            # å¯é€‰ï¼šåŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæ–­ç‚¹ç»­è®­éœ€è¦ï¼‰
            if 'opt_gen_state_dict' in checkpoint:
                self.opt_gen.load_state_dict(checkpoint['opt_gen_state_dict'])
            if 'opt_disc_state_dict' in checkpoint:
                self.opt_disc.load_state_dict(checkpoint['opt_disc_state_dict'])
            # æ¢å¤æœ€ä½³PSNRå’Œèµ·å§‹epoch
            self.best_psnr = checkpoint.get('best_psnr', self.best_psnr)
            start_epoch = checkpoint.get('epoch', 0) + 1  # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
            print(f"âœ… åŠ è½½æ£€æŸ¥ç‚¹æˆåŠŸï¼š{checkpoint_path}")
            print(f"  - æ¢å¤åˆ°Epoch: {start_epoch}")
            print(f"  - å†å²æœ€ä½³PSNRï¼ˆç§˜å¯†ï¼‰ï¼š{self.best_psnr:.2f} dB")
            return start_epoch
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼š{checkpoint_path}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            return 0

    def save_sample_images(self, epoch=None, num_samples=5):
        """ä¿å­˜ç¤ºä¾‹å›¾ç‰‡ï¼šå®¿ä¸»ã€å®¹å™¨ã€åŸç§˜å¯†ã€æå–çš„ç§˜å¯†"""
        self.gen.eval()
        save_dir = os.path.join(c.SAMPLE_IMAGE_PATH, f"epoch_{epoch}" if epoch else "final")
        os.makedirs(save_dir, exist_ok=True)

        with torch.no_grad():
            for idx, (host, secret) in enumerate(self.val_loader):
                if idx >= num_samples:
                    break  # åªä¿å­˜æŒ‡å®šæ•°é‡çš„æ ·æœ¬

                # å‰å‘ä¼ æ’­
                host = host.to(self.device)
                secret = secret.to(self.device)
                container = self.gen.embed(host, secret)
                extracted_secret = self.gen.extract(container)
                # è½¬æ¢ä¸º[0,1]èŒƒå›´åä¼ å…¥refine_head
                extracted_secret_01 = (extracted_secret + 1) / 2
                extracted_refined = self.refine_head(extracted_secret_01)

                # è½¬æ¢ä¸º[0, 1]èŒƒå›´ï¼ˆé€‚é…å›¾åƒä¿å­˜ï¼‰
                def tensor_to_01(tensor):
                    return (tensor.cpu() + 1) / 2  # ä»[-1,1]è½¬[0,1]

                # å–batchä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆç»´åº¦ï¼šCÃ—HÃ—Wï¼‰
                host_01 = tensor_to_01(host[0])
                container_01 = tensor_to_01(container[0])
                secret_01 = tensor_to_01(secret[0])
                # refinedæ˜¯[0,1]èŒƒå›´ï¼Œç›´æ¥è½¬æ¢
                extracted_refined_01 = extracted_refined[0].cpu().clamp(0, 1)

                # è½¬æ¢ä¸ºPILå›¾åƒï¼ˆå¤„ç†å•é€šé“/ä¸‰é€šé“ï¼‰
                def pil_convert(tensor):
                    if tensor.shape[0] == 1:  # å•é€šé“ï¼ˆç°åº¦å›¾ï¼‰
                        return Image.fromarray((tensor.squeeze(0).numpy() * 255).astype(np.uint8), mode='L')
                    else:  # ä¸‰é€šé“ï¼ˆRGBå›¾ï¼‰
                        return Image.fromarray((tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8), mode='RGB')

                # ä¿å­˜å•ä¸ªå›¾åƒ
                pil_convert(host_01).save(os.path.join(save_dir, f"sample_{idx+1}_host.png"))
                pil_convert(container_01).save(os.path.join(save_dir, f"sample_{idx+1}_container.png"))
                pil_convert(secret_01).save(os.path.join(save_dir, f"sample_{idx+1}_secret_ori.png"))
                pil_convert(extracted_refined_01).save(os.path.join(save_dir, f"sample_{idx+1}_secret_ext.png"))

                # æ‹¼æ¥å›¾åƒå¹¶ä¿å­˜ï¼ˆæ›´ç›´è§‚å¯¹æ¯”ï¼‰
                fig, axes = plt.subplots(2, 2, figsize=(12, 12))
                axes[0, 0].imshow(pil_convert(host_01))
                axes[0, 0].set_title("Host Image", fontsize=12)
                axes[0, 0].axis('off')

                axes[0, 1].imshow(pil_convert(container_01))
                axes[0, 1].set_title("Container Image", fontsize=12)
                axes[0, 1].axis('off')

                axes[1, 0].imshow(pil_convert(secret_01))
                axes[1, 0].set_title("Original Secret", fontsize=12)
                axes[1, 0].axis('off')

                axes[1, 1].imshow(pil_convert(extracted_refined_01))
                axes[1, 1].set_title("Refined Extracted Secret", fontsize=12)
                axes[1, 1].axis('off')

                plt.tight_layout()
                plt.savefig(os.path.join(save_dir, f"sample_{idx+1}_combined.png"), dpi=300, bbox_inches='tight')
                plt.close()

        print(f"ğŸ“¸ ç¤ºä¾‹å›¾ç‰‡å·²ä¿å­˜è‡³ï¼š{save_dir}")

    def train_one_epoch(self, epoch):
        """è®­ç»ƒå•ä¸ªEpoch"""
        self.gen.train()
        self.disc.train()
        total_loss_gen = 0.0
        total_loss_disc = 0.0

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch + 1}/{c.epochs}")
        for host, secret in pbar:
            host = host.to(self.device)
            secret = secret.to(self.device)
            batch_size = host.shape[0]

            # -------------------- è®­ç»ƒåˆ¤åˆ«å™¨ --------------------
            self.opt_disc.zero_grad()
            # ç”Ÿæˆå®¹å™¨å›¾åƒï¼ˆ-1~1èŒƒå›´ï¼‰
            container = self.gen.embed(host, secret)
            # è½¬æ¢ä¸º[0,1]èŒƒå›´ï¼ˆé€‚é…æ”»å‡»ã€åˆ¤åˆ«å™¨ã€æŒ‡æ ‡è®¡ç®—ï¼‰
            container_01 = (container + 1) / 2
            host_01 = (host + 1) / 2
            # ä¿ç•™æ¢¯åº¦ï¼ˆæ”»å‡»éœ€è¦ï¼‰
            container_01.retain_grad()

            # åˆ¤åˆ«å™¨æ ‡ç­¾ï¼ˆçœŸå®=1ï¼Œç”Ÿæˆ=0ï¼‰
            real_label = torch.ones(batch_size, device=self.device)
            fake_label = torch.zeros(batch_size, device=self.device)

            # åˆ¤åˆ«å™¨æŸå¤±ï¼šä½¿ç”¨Main.Discriminatorçš„get_scoreæ–¹æ³•
            loss_real = self.bce_loss(self.disc.get_score(host_01), real_label)
            loss_fake = self.bce_loss(self.disc.get_score(container_01.detach()), fake_label)
            loss_disc = (loss_real + loss_fake) * 0.5

            # æ–°å¢ï¼šåˆ¤åˆ«å™¨ç‰¹å¾åŒ¹é…æŸå¤±ï¼ˆå¢å¼ºåˆ¤åˆ«å™¨å­¦ä¹ ï¼‰
            if hasattr(self.disc, 'get_features'):
                real_feats = self.disc.get_features(host_01)
                fake_feats = self.disc.get_features(container_01.detach())
                loss_disc_feat = self.get_feature_matching_loss(real_feats, fake_feats)
                loss_disc += loss_disc_feat * c.feat_match_weight  # ç‰¹å¾åŒ¹é…æŸå¤±æƒé‡

            # åå‘ä¼ æ’­+ä¼˜åŒ–
            loss_disc.backward()
            self.opt_disc.step()
            total_loss_disc += loss_disc.item()

            # -------------------- è®­ç»ƒç”Ÿæˆå™¨ --------------------
            self.opt_gen.zero_grad()
            # 1. ç”Ÿæˆå¼è¯¯å·®é¢„åˆ¤ï¼šç”¨é¢„æµ‹è¯¯å·®ä¿®æ­£å®¹å™¨å›¾åƒï¼ˆä¼˜åŒ–åˆ›æ–°ç‚¹3ï¼‰
            pred_error_total = None
            if self.training:
                # è§£åŒ…è¿”å›çš„å…ƒç»„ï¼Œåªå–æ€»è¯¯å·®ï¼ˆç¬¬ä¸€ä¸ªå€¼ï¼‰
                pred_error_total, _, _, _ = self.error_predictor(container)
                # ç”¨æ€»è¯¯å·®è¿›è¡Œä¿®æ­£
                container = container - c.error_correction_weight * pred_error_total
                container = torch.clamp(container, -1.0, 1.0)  # é™åˆ¶èŒƒå›´
                # é‡æ–°è½¬æ¢ä¸º[0,1]ï¼ˆä¿®æ­£åï¼‰
                container_01 = (container + 1) / 2
                container_01.retain_grad()  # é‡æ–°ä¿ç•™æ¢¯åº¦

            # 2. å¯¹æŠ—æ”»å‡»ï¼ˆéšæœºé€‰æ‹©æ”»å‡»ç±»å‹ï¼‰
            attack_type = np.random.choice(c.supported_attacks)
            container_attacked_01 = None
            if attack_type in ['fgsm', 'pgd']:
                # å¯¹æŠ—æ”»å‡»éœ€è¦4ç»´å¼ é‡çš„æ¢¯åº¦ï¼Œè°ƒç”¨discçš„forwardï¼ˆè¿”å›4ç»´ï¼‰
                pred_4d = self.disc(container_01)  # 4ç»´å¼ é‡ï¼š(batch, 1, h, w)
                # è®¡ç®—æŸå¤±æ—¶ç”¨å…¨å±€å¹³å‡ï¼ˆ1ç»´ï¼‰
                loss_gan_temp = self.bce_loss(pred_4d.mean(dim=[1, 2, 3]), real_label)
                loss_gan_temp.backward(retain_graph=True)
                # å®‰å…¨è·å–æ¢¯åº¦
                container_grad = container_01.grad.detach() if container_01.grad is not None else torch.zeros_like(container_01)
                # æ‰§è¡Œæ”»å‡»
                container_attacked_01 = attack_image(container_01, attack_type, container_grad, c.epsilon)
                # æ¸…ç©ºæ¢¯åº¦ï¼Œé¿å…æ®‹ç•™
                container_01.grad.zero_()
            else:
                # éå¯¹æŠ—æ”»å‡»
                container_attacked_01 = attack_image(container_01, attack_type)

            # è½¬æ¢å›[-1,1]èŒƒå›´
            container_attacked = (container_attacked_01 * 2) - 1

            # 3. æå–å¹¶ä¿®å¤ç§˜å¯†
            extracted_secret = self.gen.extract(container_attacked)
            # è½¬æ¢ä¸º[0,1]èŒƒå›´åä¼ å…¥refine_headï¼ˆåŒ¹é…Mainçš„refine_headé¢„æœŸï¼‰
            extracted_secret_01 = (extracted_secret + 1) / 2
            extracted_secret_refined = self.refine_head(extracted_secret_01)  # refine_headè¾“å‡º[0,1]

            # 4. åŠ¨æ€æƒé‡è°ƒæ•´ï¼ˆä¿®å¤é”®ä¸åŒ¹é…é—®é¢˜+å¼ºåˆ¶æå‡å®¹é‡æƒé‡ï¼‰
            texture_complexity = self.pzms_extractor.get_texture_complexity(host)
            # è½¬æ¢ä¸º[0,1]èŒƒå›´ï¼ˆæŸå¤±è®¡ç®—ï¼‰
            secret_01 = (secret + 1) / 2
            # è®¡ç®—åŸºç¡€æŸå¤±ï¼ˆé”®æ”¹ä¸ºcap/rob/impï¼ŒåŒ¹é…weight_balancerï¼‰
            # æ ¸å¿ƒæ”¹è¿›ï¼šæé«˜å®¹é‡æŸå¤±çš„åŸºç¡€æƒé‡ï¼ˆÃ—5ï¼‰ï¼Œä¼˜å…ˆä¿è¯ç§˜å¯†æå–
            loss_capacity = self.mse_loss(extracted_secret_refined, secret_01) * 5.0  # å®¹é‡æŸå¤±Ã—5
            loss_imperceptible = 1 - SSIM(container_01, host_01)
            loss_robustness = self.mse_loss(extracted_secret_01, extracted_secret_refined)
            # æ”¶é›†å½“å‰æŸå¤±å€¼ï¼ˆé”®åŒ¹é…ï¼‰
            current_losses = {
                'cap': loss_capacity.item(),
                'rob': loss_robustness.item(),
                'imp': loss_imperceptible.item()
            }
            # åŠ¨æ€è°ƒæ•´æƒé‡
            cap_w, rob_w, imp_w = self.weight_balancer.adjust(texture_complexity, attack_type, current_losses)

            # æ–°å¢ï¼šå½“ç§˜å¯†æå–æŸå¤±è¿‡é«˜æ—¶ï¼Œå¼ºåˆ¶æå‡å®¹é‡æƒé‡
            if loss_capacity.item() > 0.1:  # é˜ˆå€¼å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                cap_w *= 2.0  # å®¹é‡æƒé‡Ã—2

            # 5. æŸå¤±è®¡ç®—
            # ä¿®å¤error_predictorçš„å…ƒç»„é—®é¢˜ï¼šä½¿ç”¨ä¹‹å‰ä¿å­˜çš„pred_error_total
            if pred_error_total is None:
                pred_error_total, _, _, _ = self.error_predictor(container)
            pred_error_01 = (pred_error_total + 1) / 2  # è½¬æ¢ä¸º[0,1]
            loss_error = self.mse_loss(pred_error_01, torch.zeros_like(container_01))

            # GANæŸå¤±ï¼šä½¿ç”¨get_scoreæ–¹æ³•ï¼ˆè¿”å›1ç»´ï¼ŒåŒ¹é…BCEæŸå¤±ï¼‰
            loss_gan = self.bce_loss(self.disc.get_score(container_01), real_label)

            # æ–°å¢ï¼šç”Ÿæˆå™¨ç‰¹å¾åŒ¹é…æŸå¤±ï¼ˆå¢å¼ºç”Ÿæˆå™¨å­¦ä¹ ï¼‰
            loss_feat_match = 0.0
            if hasattr(self.disc, 'get_features'):
                real_feats = self.disc.get_features(host_01)
                fake_feats = self.disc.get_features(container_01)
                loss_feat_match = self.get_feature_matching_loss(real_feats, fake_feats)

            # æ€»æŸå¤±ï¼ˆåŠ æƒæ±‚å’Œï¼‰ï¼šæ·»åŠ ç‰¹å¾åŒ¹é…æŸå¤±
            loss_gen = (
                cap_w * loss_capacity +
                rob_w * loss_robustness +
                imp_w * loss_imperceptible +
                c.gan_weight * loss_gan +
                c.error_pred_weight * loss_error +
                c.feat_match_weight * loss_feat_match  # ç‰¹å¾åŒ¹é…æŸå¤±
            )

            # åå‘ä¼ æ’­+ä¼˜åŒ–
            loss_gen.backward()
            self.opt_gen.step()
            total_loss_gen += loss_gen.item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                "Gen Loss": f"{loss_gen.item():.4f}",
                "Disc Loss": f"{loss_disc.item():.4f}"
            })

        # å¹³å‡æŸå¤±
        avg_loss_gen = total_loss_gen / len(self.train_loader)
        avg_loss_disc = total_loss_disc / len(self.train_loader)
        # è®°å½•æ—¥å¿—
        self.writer.add_scalar("Train/Gen_Loss", avg_loss_gen, epoch)
        self.writer.add_scalar("Train/Disc_Loss", avg_loss_disc, epoch)
        print(f"\nEpoch {epoch + 1} | Gen Loss: {avg_loss_gen:.4f} | Disc Loss: {avg_loss_disc:.4f}")

        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler_gen.step()
        self.scheduler_disc.step()

    def validate(self, epoch):
        """éªŒè¯é˜¶æ®µï¼šè®¡ç®—PSNR/SSIMå¹¶ä¿å­˜æœ€ä½³æ¨¡å‹"""
        self.gen.eval()
        total_psnr_secret = 0.0  # ç§˜å¯†æå–çš„PSNR
        total_ssim_secret = 0.0  # ç§˜å¯†æå–çš„SSIM
        total_psnr_container = 0.0  # å®¹å™¨ä¸å®¿ä¸»çš„PSNRï¼ˆä¸å¯æ„ŸçŸ¥æ€§ï¼‰
        total_ssim_container = 0.0  # å®¹å™¨ä¸å®¿ä¸»çš„SSIMï¼ˆä¸å¯æ„ŸçŸ¥æ€§ï¼‰

        with torch.no_grad():
            for host, secret in self.val_loader:
                host = host.to(self.device)
                secret = secret.to(self.device)

                # ç”Ÿæˆå®¹å™¨å¹¶æå–ç§˜å¯†
                container = self.gen.embed(host, secret)
                # è½¬æ¢ä¸º[0,1]èŒƒå›´
                container_01 = (container + 1) / 2
                host_01 = (host + 1) / 2
                secret_01 = (secret + 1) / 2

                # æå–å¹¶ä¿®å¤ç§˜å¯†
                extracted_secret = self.gen.extract(container)
                extracted_secret_01 = (extracted_secret + 1) / 2
                extracted_refined = self.refine_head(extracted_secret_01)

                # è®¡ç®—æŒ‡æ ‡ï¼šç§˜å¯†æå–ï¼ˆåŸé€»è¾‘ï¼‰
                total_psnr_secret += PSNR(extracted_refined, secret_01)
                total_ssim_secret += SSIM(extracted_refined, secret_01).item()
                # è®¡ç®—æŒ‡æ ‡ï¼šå®¹å™¨ä¸å®¿ä¸»ï¼ˆä¸å¯æ„ŸçŸ¥æ€§ï¼Œæ–°å¢ï¼‰
                total_psnr_container += PSNR(container_01, host_01)
                total_ssim_container += SSIM(container_01, host_01).item()

        # å¹³å‡æŒ‡æ ‡
        avg_psnr_secret = total_psnr_secret / len(self.val_loader)
        avg_ssim_secret = total_ssim_secret / len(self.val_loader)
        avg_psnr_container = total_psnr_container / len(self.val_loader)
        avg_ssim_container = total_ssim_container / len(self.val_loader)

        # è®°å½•æ—¥å¿—
        self.writer.add_scalar("Val/PSNR_Secret", avg_psnr_secret, epoch)
        self.writer.add_scalar("Val/SSIM_Secret", avg_ssim_secret, epoch)
        self.writer.add_scalar("Val/PSNR_Container", avg_psnr_container, epoch)
        self.writer.add_scalar("Val/SSIM_Container", avg_ssim_container, epoch)

        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š Validation Results:")
        print(f"  - Secret: PSNR = {avg_psnr_secret:.2f} dB | SSIM = {avg_ssim_secret:.4f}")
        print(f"  - Container: PSNR = {avg_psnr_container:.2f} dB | SSIM = {avg_ssim_container:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä»¥ç§˜å¯†æå–çš„PSNRä¸ºæŒ‡æ ‡ï¼Œè¦æ±‚è‡³å°‘æå‡2%ï¼‰
        if avg_psnr_secret > self.best_psnr * self.psnr_improve_threshold:
            # é‡ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼šéªŒè¯æŒ‡æ ‡æå‡æ—¶é‡ç½®å­¦ä¹ ç‡ï¼‰
            self.scheduler_gen.last_epoch = -1
            self.scheduler_disc.last_epoch = -1
            print(f"ğŸ”„ éªŒè¯PSNRæå‡è¶…è¿‡{self.psnr_improve_threshold-1:.0%}ï¼Œé‡ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨")

            self.best_psnr = avg_psnr_secret
            self.best_psnr_container = avg_psnr_container
            # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­ï¼‰
            torch.save({
                "gen_state_dict": self.gen.state_dict(),
                "disc_state_dict": self.disc.state_dict(),
                "opt_gen_state_dict": self.opt_gen.state_dict(),
                "opt_disc_state_dict": self.opt_disc.state_dict(),
                "epoch": epoch,
                "best_psnr": self.best_psnr,
                "best_psnr_container": self.best_psnr_container
            }, os.path.join(c.CHECKPOINT_PATH, "best_model.pth"))
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆSecret PSNR: {avg_psnr_secret:.2f} dBï¼‰")

        return avg_psnr_secret  # è¿”å›PSNRç”¨äºæ—©åœåˆ¤æ–­

    def run(self):
        """å¯åŠ¨è®­ç»ƒæµç¨‹ï¼ˆå«æ—©åœã€æ–­ç‚¹ç»­è®­ã€å›¾ç‰‡ä¿å­˜ï¼‰"""
        # æ­¥éª¤1ï¼šåŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆç¬¬äºŒæ¬¡è®­ç»ƒæ—¶è‡ªåŠ¨ä½¿ç”¨ï¼‰
        start_epoch = self.load_checkpoint(os.path.join(c.CHECKPOINT_PATH, "best_model.pth"))

        # æ­¥éª¤2ï¼šåˆå§‹åŒ–æ—©åœå‚æ•°
        no_improve_epochs = 0
        early_stop_triggered = False

        # æ­¥éª¤3ï¼šå¼€å§‹è®­ç»ƒ
        for epoch in range(start_epoch, c.epochs):
            # è®­ç»ƒå•ä¸ªepoch
            self.train_one_epoch(epoch)

            # éªŒè¯+ä¿å­˜ç¤ºä¾‹å›¾ç‰‡
            if (epoch + 1) % c.val_freq == 0:
                avg_psnr = self.validate(epoch)
                # ä¿å­˜ç¤ºä¾‹å›¾ç‰‡
                self.save_sample_images(epoch=epoch+1)

                # æ”¹è¿›æ—©åœåˆ¤æ–­ï¼šåŠ¨æ€è€å¿ƒå€¼+æ¯”ä¾‹æå‡åˆ¤æ–­
                # åŠ¨æ€è€å¿ƒå€¼ï¼šè®­ç»ƒåæœŸï¼ˆè¶…è¿‡ä¸€åŠepochï¼‰è€å¿ƒå€¼å‡åŠ
                current_patience = c.early_stop_patience if epoch < c.epochs//2 else c.early_stop_patience // 2

                if avg_psnr > self.best_psnr * self.psnr_improve_threshold:
                    no_improve_epochs = 0  # é‡ç½®è®¡æ•°å™¨ï¼ˆä»…å½“æœ‰æ•ˆæå‡æ—¶ï¼‰
                else:
                    no_improve_epochs += 1
                    print(f"âš ï¸  éªŒè¯PSNRè¿ç»­{no_improve_epochs}è½®æœªæ˜¾è‘—æå‡ï¼ˆè€å¿ƒï¼š{current_patience}ï¼‰")
                    if no_improve_epochs >= current_patience:
                        print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒï¼ˆEpoch: {epoch+1}ï¼‰")
                        early_stop_triggered = True
                        break

            # ä¿å­˜æ–­ç‚¹
            if (epoch + 1) % c.save_freq == 0 and not early_stop_triggered:
                torch.save({
                    "gen_state_dict": self.gen.state_dict(),
                    "disc_state_dict": self.disc.state_dict(),
                    "opt_gen_state_dict": self.opt_gen.state_dict(),
                    "opt_disc_state_dict": self.opt_disc.state_dict(),
                    "epoch": epoch,
                    "best_psnr": self.best_psnr
                }, os.path.join(c.CHECKPOINT_PATH, f"model_epoch_{epoch + 1}.pth"))
                print(f"ğŸ’¾ ä¿å­˜æ–­ç‚¹æ¨¡å‹ï¼šmodel_epoch_{epoch+1}.pth")

        # è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆç¤ºä¾‹å›¾ç‰‡
        if not early_stop_triggered:
            self.save_sample_images()  # ä¿å­˜finalç‰ˆæœ¬

        # å…³é—­æ—¥å¿—å†™å…¥å™¨
        self.writer.close()
        print("\nğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆï¼")

if __name__ == "__main__":
    # å®ä¾‹åŒ–å¹¶å¯åŠ¨è®­ç»ƒ
    trainer = Trainer()
    trainer.run()
